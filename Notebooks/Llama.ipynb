{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "plFroaGkxr7n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers.image_utils import load_image\n",
    "import torch\n",
    "import csv\n",
    "import platform\n",
    "from transformers import pipeline\n",
    "import psutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "beff6fe1bbfd489c88b4d0f9940a1d01",
      "6872ab4ee023411b83902a76159f51a5",
      "03fb4a86fd634888a5b7b79b8de644c0",
      "6b414c9df53d4cff91d850c1ec206af4",
      "24bdb648a9924a1e9c7492bfad303051",
      "e320a6d270f944fcb0002ef0f8316f55",
      "1c8a0accbd9e4ecd92ea6759abcb90b8",
      "ac395c5cf8984c588b5aec4b16d3ecd7",
      "b2b2c799c6424b3d8c04b6b0d740495f",
      "c14df69eb5b34254aab962df8ff25261",
      "35e83eef2b3942de9988b141242abf26"
     ]
    },
    "id": "hdmdVbNBxaZl",
    "outputId": "4e6c4c7a-c491-4f59-a7f2-6d9f04ecd6e9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# model_id = [\"unsloth/Llama-3.2-11B-Vision-Instruct\"]\n",
    "# model_id = [\"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\"]\n",
    "\n",
    "model_list = [\n",
    "    \n",
    "     # Can fit in a 80GB card!\n",
    "\n",
    "               \n",
    "\n",
    "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
    "\n",
    "     \"unsloth/Pixtral-12B-2409-bnb-4bit\"\n",
    "]\n",
    "\n",
    "model_id = [\"unsloth/Pixtral-12B-2409-bnb-4bit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L10lGQF8yCdE"
   },
   "outputs": [],
   "source": [
    "# Image directory\n",
    "img_dir = 'benchmark_vlm/master'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8k86dXQJ9_81"
   },
   "source": [
    "### Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.9: Fast Mistral vision patching. Transformers: 4.48.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.256 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262ba7beb4c444f0ab52ed17bd43e1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a805748e6ead4ad794921551ceefac89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecdd34ff8e3446cba9aa78992418cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188e05a96403410b8102514167508b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aead0aa9eae44f8935f3ae99ef4df9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/133 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6921df4aefac4bc09b03b6242bbaf2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/162 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f35c598bdcf42dc8b82018ebceda9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01838ef3046c4028ac6407603ed28029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff3d34fae964f5882a0ad3e4fa14931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/177k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87110b59872d49de90facc2fd706a2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fff5b1036245c1aa4ac0572803d731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastVisionModel.from_pretrained(model_id[0], load_in_4bit = True, use_gradient_checkpointing = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iDIEGC215lsG"
   },
   "outputs": [],
   "source": [
    "# Function to run inference\n",
    "def infer_image(image_path, prompt):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "          input_image = Image.open(f).convert(\"RGB\")\n",
    "\n",
    "    \n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": prompt}\n",
    "    ]}\n",
    "]\n",
    "   \n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "    inputs = tokenizer( input_image, input_text, add_special_tokens = True, return_tensors = \"pt\",).to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=120)\n",
    "    print(tokenizer.decode(output[0]))\n",
    "    return tokenizer.decode(output[0]).split()[-1].split('.')[0].lower()\n",
    "\n",
    "    \n",
    "    # return (tokenizer.decode(output[0]).split()[-2].lower())\n",
    "\n",
    "\n",
    "#llama = (tokenizer.decode(output[0]).split()[-1].split('.')[0].lower())\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:93\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minfer_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbenchmark_vlm/master/Class 5 - 3 axle truck, bus/truck 3/165_jpg.rf.0a6d8c9e33bab77bf8be134e5163fac2.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIs this a pickup truck? Respond with only yes or no.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36minfer_image\u001b[0;34m(image_path, prompt)\u001b[0m\n\u001b[1;32m     14\u001b[0m input_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, add_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer( input_image, input_text, add_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:2029\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   2026\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   2027\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   2028\u001b[0m )\n\u001b[0;32m-> 2029\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2031\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:95\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "infer_image('benchmark_vlm/master/Class 5 - 3 axle truck, bus/truck 3/165_jpg.rf.0a6d8c9e33bab77bf8be134e5163fac2.jpg', 'Is this a pickup truck? Respond with only yes or no.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kymovqsQ99A_"
   },
   "source": [
    "### Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NM8O5ht0-G9x"
   },
   "outputs": [],
   "source": [
    "# Define vehicle classes for validation\n",
    "vehicle_classes = {\n",
    "    \"Class 1 - Motorcycles\": [\"motorcycle\", \"motorbike\", \"bike\", \"scooter\", \"motor\", \"moped\"],\n",
    "    \"Class 2 - Car, SUV, Station Wagons\": [\"car\", \"sedan\", \"hatchback\", \"suv\", \"mpv\", \"crossover\", \"station wagon\", \"wagon\", 'coupe','coupÃ©', 'convertible', 'sports car'],\n",
    "    \"Class 3 - 2 axle lorry, van, pickup\": [\"pickup\", \"van\", \"minivan\"],\n",
    "    \"Class 4 - 2 axles, 5 or 6 wheels bus or truck\": [\"bus\", \"truck\", 'trailer', 'lorry', 'tractor', 'tanker', 'dump truck', \"tow truck\", \"dump trailer\"],\n",
    "    \"Class 5 - 3 axle truck, bus\": [\"bus\", \"truck\"],\n",
    "    \"Class 6 - 4 axles\": [\"coach\", \"bus\", \"truck\", 'trailer', 'lorry', 'tractor', 'tanker', 'dump truck', \"tow truck\", \"dump trailer\"],\n",
    "    \"Class 7 - 5 or more axles\": [\"coach\", \"bus\", \"truck\", 'trailer', 'lorry', 'tractor', 'tanker', 'dump truck', \"tow truck\", \"dump trailer\"],\n",
    "    \"bus 2\" : [\"coach\", \"bus\"],\n",
    "    \"bus 3\" : [\"coach\", \"bus\"],\n",
    "    \"truck 2\" : [\"truck\", 'trailer', 'lorry', 'tractor', 'tanker', 'dump truck', \"tow truck\", \"dump trailer\"],\n",
    "    \"truck 3\" : [\"truck\", 'trailer', 'lorry', 'tractor', 'tanker', 'dump truck', \"tow truck\", \"dump trailer\"],\n",
    "    \"truck 4\" : [\"truck\", 'trailer', 'lorry', 'tractor', 'tanker', 'dump truck', \"tow truck\", \"dump trailer\"],\n",
    "    \"truck 5\" : [\"truck\", 'trailer', 'lorry', 'tractor', 'tanker', 'dump truck', \"tow truck\", \"dump trailer\"],\n",
    "    \"van\" : [\"van\", \"minivan\", 'minibus', 'mpv'],\n",
    "    \"pickup\" : [\"pickup\", \"pickup truck\", \"camper\"],\n",
    "}\n",
    "\n",
    "valid_vehicle_types = {type for sublist in vehicle_classes.values() for type in sublist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "thh669gy5oLj"
   },
   "outputs": [],
   "source": [
    "# Validate predictions\n",
    "\n",
    "def validate_prediction(folder_name, prediction, axles=None):\n",
    "    # Extract folder label\n",
    "    for class_name, valid_labels in vehicle_classes.items():\n",
    "        if folder_name.startswith(class_name):\n",
    "            if axles:  # Use axle count for bus/truck\n",
    "                if axles == 1 or 0:\n",
    "                  return False\n",
    "                elif axles == 2:\n",
    "                    return \"2\" in folder_name[-1]\n",
    "                elif axles == 3:\n",
    "                    return \"3\" in folder_name[-1]\n",
    "                elif axles == 4:\n",
    "                    return \"4\" in folder_name[-1]\n",
    "                elif axles >= 5:\n",
    "                    return \"5\" in folder_name[-1]\n",
    "            if isinstance(valid_labels, list):\n",
    "                return prediction in valid_labels\n",
    "            return prediction == valid_labels\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYX4uILS97b0"
   },
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "as1UY4lBj0Ha",
    "outputId": "5a7878a3-4ca6-4958-a2b8-646dfb69a5ac"
   },
   "outputs": [],
   "source": [
    "def main_loop(wrong_predictions_dir, img_dir=img_dir, use_axles=False, valid_vehicle_types=valid_vehicle_types):\n",
    "    total_files = sum(len(files) for _, _, files in os.walk(img_dir))\n",
    "    progress_bar = tqdm(total=total_files, desc=\"Processing Images\", unit=\"file\")\n",
    "    # Main processing loop\n",
    "    start_time = time.time()\n",
    "    tracker = EmissionsTracker()\n",
    "    tracker.start()\n",
    "    results = []\n",
    "    for root, dirs, files in os.walk(img_dir):\n",
    "        for file in files:\n",
    "            if file.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "                image_path = os.path.join(root, file)\n",
    "                folder_name = os.path.basename(os.path.dirname(image_path))\n",
    "                progress_bar.set_description(f\"Processing: {folder_name}\")\n",
    "                progress_bar.update(1)  # Increment the progress bar\n",
    "\n",
    "                try:\n",
    "                    # First inference: vehicle type\n",
    "                    # vehicle_prompt = f\"<image> answer en What type of vehicle is this? Answer in only one word.\"\n",
    "                    vehicle_prompt = f\"What type of vehicle is this? Choose one from: {', '.join(valid_vehicle_types)}. Answer in one word\"\n",
    "                    prediction = infer_image(image_path, vehicle_prompt)\n",
    "                    print(f\"Vehicle Prediction: {prediction}\")\n",
    "\n",
    "                    # Handle buses and trucks\n",
    "                    axles = None\n",
    "                    if prediction in [\"bus\", \"truck\", 'trailer', 'lorry', 'tractor', 'tanker', 'dump truck',  \"tow truck\", \"dump trailer\"]:\n",
    "                        # Check if it's a pickup truck\n",
    "                        if prediction == 'truck':\n",
    "                            pickup_prompt = \"Is this a pickup truck? Respond with only yes or no.\"\n",
    "                            is_pickup = infer_image(image_path, pickup_prompt).strip().lower()\n",
    "                            print(f\"Is Pickup Truck: {is_pickup}\")\n",
    "                            if is_pickup == \"yes\":\n",
    "                                prediction = \"pickup truck\"\n",
    "                            elif use_axles:\n",
    "                                # Predict axles for non-pickup trucks\n",
    "                                axle_prompt = \"How many axles does this vehicle have? Output a single integer value.\"\n",
    "                                axle_prediction = infer_image(image_path, axle_prompt)\n",
    "                                print(f\"Axle Prediction: {axle_prediction}\")\n",
    "                    \n",
    "                                try:\n",
    "                                    axles = int(axle_prediction)\n",
    "                                except ValueError:\n",
    "                                    print(f\"Invalid axle prediction: {axle_prediction}\")\n",
    "                                    axles = None  # Fallback if parsing fails\n",
    "                        elif use_axles:\n",
    "\n",
    "                            axle_prompt = \"How many axles does this vehicle have? Output a single integer value\"\n",
    "                            axle_prediction = infer_image(image_path, axle_prompt)\n",
    "                            print(f\"Axle Prediction: {axle_prediction}\")\n",
    "    \n",
    "                            try:\n",
    "                                axles = int(axle_prediction)\n",
    "                            except ValueError:\n",
    "                                print(f\"Invalid axle prediction: {axle_prediction}\")\n",
    "\n",
    "                    # Validate prediction\n",
    "                    is_correct = validate_prediction(folder_name, prediction, axles)\n",
    "                    print(f\"Validation Result: {is_correct}\")\n",
    "\n",
    "                    # Store results\n",
    "                    results.append({\n",
    "                        \"image\": image_path,\n",
    "                        \"folder_label\": folder_name,\n",
    "                        \"prediction\": prediction,\n",
    "                        \"axles\": axles,\n",
    "                        \"is_correct\": is_correct\n",
    "                    })\n",
    "\n",
    "                    # Handle wrong predictions\n",
    "                    if not is_correct:\n",
    "                        # Create a folder for the wrong prediction\n",
    "                        wrong_folder = os.path.join(wrong_predictions_dir, prediction, str(axles))\n",
    "                        os.makedirs(wrong_folder, exist_ok=True)\n",
    "\n",
    "                        # Copy the image to the folder\n",
    "                        shutil.copy(image_path, os.path.join(wrong_folder, file))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    progress_bar.close()\n",
    "    emissions = tracker.stop()\n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total processing time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"CO2 emissions: {emissions:.6f} kg\")\n",
    "    return (results, elapsed_time, emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4COQqG95sqN",
    "outputId": "59ceb760-001c-4faf-cf3d-740399491993"
   },
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "def export_results(results, model_id, elapsed_time, emissions):\n",
    "# Save results to CSV\n",
    "    output_csv = f'experiment_data/{models}/results.csv'\n",
    "    with open(output_csv, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"image\", \"folder_label\", \"prediction\", \"axles\", \"is_correct\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    # Metrics calculation\n",
    "    correct_predictions = sum(1 for r in results if r[\"is_correct\"])\n",
    "    total_images = len(results)\n",
    "    accuracy = correct_predictions / total_images * 100 if total_images > 0 else 0\n",
    "    wrong_predictions = total_images - correct_predictions\n",
    "\n",
    "    # Per-class metrics\n",
    "    class_metrics = {}\n",
    "    for result in results:\n",
    "        label = result[\"folder_label\"]\n",
    "        correct = result[\"is_correct\"]\n",
    "        if label not in class_metrics:\n",
    "            class_metrics[label] = {\"correct\": 0, \"total\": 0}\n",
    "        class_metrics[label][\"total\"] += 1\n",
    "        if correct:\n",
    "            class_metrics[label][\"correct\"] += 1\n",
    "\n",
    "    # Aggregate class metrics\n",
    "    per_class_accuracy = {\n",
    "        label: (data[\"correct\"] / data[\"total\"]) * 100\n",
    "        for label, data in class_metrics.items()\n",
    "    }\n",
    "    per_class_error_rate = {\n",
    "        label: 100 - acc for label, acc in per_class_accuracy.items()\n",
    "    }\n",
    "\n",
    "    # Collect system configuration\n",
    "    system_config = {\n",
    "        \"Processor\": platform.processor(),\n",
    "        \"CPU Count\": psutil.cpu_count(logical=True),\n",
    "        \"Memory (GB)\": round(psutil.virtual_memory().total / (1024 ** 3), 2),\n",
    "        \"Elapsed Time (seconds)\": elapsed_time,\n",
    "        \"CO2 Emissions (kg)\": round(emissions, 6)\n",
    "    }\n",
    "\n",
    "    # Metrics dictionarya\n",
    "    metrics = {\n",
    "        \"Total Images\": total_images,\n",
    "        \"Correct Predictions\": correct_predictions,\n",
    "        \"Wrong Predictions\": wrong_predictions,\n",
    "        \"Accuracy (%)\": accuracy,\n",
    "        \"Error Rate (%)\": 100 - accuracy\n",
    "    }\n",
    "\n",
    "    # Save metrics to CSV\n",
    "    metrics_csv = f'experiment_data/{models}/metrics.csv'\n",
    "    with open(metrics_csv, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Model\", \"Metric\", \"Value\"])\n",
    "        for key, value in metrics.items():\n",
    "            writer.writerow([model_id, key, value])\n",
    "        for label, acc in per_class_accuracy.items():\n",
    "            writer.writerow([model_id, f\"Per-Class Accuracy: {label}\", acc])\n",
    "            writer.writerow([model_id, f\"Per-Class Error Rate: {label}\", per_class_error_rate[label]])\n",
    "        for key, value in system_config.items():\n",
    "            writer.writerow([model_id, key, value])\n",
    "\n",
    "    # Print final metrics\n",
    "    print(\"\\n--- Metrics ---\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    for label, acc in per_class_accuracy.items():\n",
    "        print(f\"Class '{label}' - Accuracy: {acc:.2f}%, Error Rate: {per_class_error_rate[label]:.2f}%\")\n",
    "    for key, value in system_config.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsloth/Llama-3.2-90B-Vision-bnb-4bit\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "unsloth/llama-3.2-90b-vision-unsloth-bnb-4bit/*.json (repository not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/unsloth/llama-3.2-90b-vision-unsloth-bnb-4bit",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:125\u001b[0m, in \u001b[0;36mHfFileSystem._repo_and_revision_exist\u001b[0;34m(self, repo_type, repo_id, revision)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_HUB_ETAG_TIMEOUT\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (RepositoryNotFoundError, HFValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2748\u001b[0m, in \u001b[0;36mHfApi.repo_info\u001b[0;34m(self, repo_id, revision, repo_type, timeout, files_metadata, expand, token)\u001b[0m\n\u001b[1;32m   2747\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported repo type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   2754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2533\u001b[0m, in \u001b[0;36mHfApi.model_info\u001b[0;34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[0m\n\u001b[1;32m   2532\u001b[0m r \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mget(path, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m-> 2533\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2534\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-676c11eb-18497e8f1fcf29f62b93192d;77a893db-46b2-452c-9d57-ba70ae416876)\n\nRepository Not Found for url: https://huggingface.co/api/models/unsloth/llama-3.2-90b-vision-unsloth-bnb-4bit.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m models \u001b[38;5;129;01min\u001b[39;00m model_id:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(models)\n\u001b[0;32m----> 3\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastVisionModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gradient_checkpointing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     wrong_predictions_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/wrong_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Directory to store wrongly predicted images\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(wrong_predictions_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/unsloth/models/loader.py:413\u001b[0m, in \u001b[0;36mFastVisionModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, return_logits, fullgraph, *args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     both_exist \u001b[38;5;241m=\u001b[39m exist_adapter_config \u001b[38;5;129;01mand\u001b[39;00m exist_config\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[43mHfFileSystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     files \u001b[38;5;241m=\u001b[39m (os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(x)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m files)\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m files) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:520\u001b[0m, in \u001b[0;36mHfFileSystem.glob\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# Set expand_info=False by default to get a x10 speed boost\u001b[39;00m\n\u001b[1;32m    519\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 520\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munresolve()\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mglob(path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:216\u001b[0m, in \u001b[0;36mHfFileSystem.resolve_path\u001b[0;34m(self, path, revision)\u001b[0m\n\u001b[1;32m    214\u001b[0m     repo_and_revision_exist, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repo_and_revision_exist(repo_type, repo_id, revision)\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_and_revision_exist:\n\u001b[0;32m--> 216\u001b[0m         \u001b[43m_raise_file_not_found\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     _raise_file_not_found(path, err)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:1136\u001b[0m, in \u001b[0;36m_raise_file_not_found\u001b[0;34m(path, err)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, HFValidationError):\n\u001b[1;32m   1135\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (invalid repository id)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: unsloth/llama-3.2-90b-vision-unsloth-bnb-4bit/*.json (repository not found)"
     ]
    }
   ],
   "source": [
    "for models in model_id:\n",
    "    print(models)\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(models, load_in_4bit = True, use_gradient_checkpointing = False)\n",
    "    wrong_predictions_dir = f'experiment_data/{models}/wrong_predictions'  # Directory to store wrongly predicted images\n",
    "    os.makedirs(wrong_predictions_dir, exist_ok=True)\n",
    "    run = main_loop(wrong_predictions_dir = wrong_predictions_dir)\n",
    "    export_results(run[0], models, run[1], run[2])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03fb4a86fd634888a5b7b79b8de644c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac395c5cf8984c588b5aec4b16d3ecd7",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b2b2c799c6424b3d8c04b6b0d740495f",
      "value": 2
     }
    },
    "1c8a0accbd9e4ecd92ea6759abcb90b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24bdb648a9924a1e9c7492bfad303051": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35e83eef2b3942de9988b141242abf26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6872ab4ee023411b83902a76159f51a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e320a6d270f944fcb0002ef0f8316f55",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1c8a0accbd9e4ecd92ea6759abcb90b8",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "6b414c9df53d4cff91d850c1ec206af4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c14df69eb5b34254aab962df8ff25261",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_35e83eef2b3942de9988b141242abf26",
      "value": "â€‡2/2â€‡[00:15&lt;00:00,â€‡â€‡7.38s/it]"
     }
    },
    "ac395c5cf8984c588b5aec4b16d3ecd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2b2c799c6424b3d8c04b6b0d740495f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "beff6fe1bbfd489c88b4d0f9940a1d01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6872ab4ee023411b83902a76159f51a5",
       "IPY_MODEL_03fb4a86fd634888a5b7b79b8de644c0",
       "IPY_MODEL_6b414c9df53d4cff91d850c1ec206af4"
      ],
      "layout": "IPY_MODEL_24bdb648a9924a1e9c7492bfad303051"
     }
    },
    "c14df69eb5b34254aab962df8ff25261": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e320a6d270f944fcb0002ef0f8316f55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
